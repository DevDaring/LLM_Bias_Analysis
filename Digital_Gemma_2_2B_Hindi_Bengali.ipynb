{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1951f2d5",
   "metadata": {},
   "source": [
    "Full Execution on Hindi Dataset for Bias Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b79e3b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script fine-tunes Gemma-2B model and performs layer-wise\n",
    "bias analysis before and after training.\n",
    "\n",
    "Workflow:\n",
    "1.  **Initial Bias Analysis**: Performs a layer-wise bias analysis on the base\n",
    "    model (`google/gemma-2b`) using an English dataset before any fine-tuning.\n",
    "2.  **Fine-tuning**: Fine-tunes the `google/gemma-2b` model on a Hindi dataset\n",
    "    (`iamshnoo/alpaca-cleaned-hindi`).\n",
    "3.  **Post-Epoch Bias Analysis**: After each fine-tuning epoch, the script runs the\n",
    "    layer-wise bias analysis on the model for both English and Hindi to track how\n",
    "    bias evolves.\n",
    "4.  **Results**: All bias analysis results are saved to CSV files for further examination.\n",
    "5.  **Upload**: After training, the script can upload the final model, tokenizer,\n",
    "    and required custom code to the Hugging Face Hub.\n",
    "\n",
    "This script can be executed with command-line arguments to specify the action\n",
    "('train' or 'upload'), training mode ('test' or 'full'), and the platform\n",
    "('colab', 'digitalocean', or 'local').\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import csv\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, login, whoami\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# =============================================================================\n",
    "BASE_MODEL_NAME = \"google/gemma-2b\"  # Changed from Llama-3.2-1B to Gemma-2B\n",
    "TOKENIZER_NAME = \"google/gemma-2b\"  # Use same tokenizer as model\n",
    "DATASET_NAME = \"iamshnoo/alpaca-cleaned-hindi\"\n",
    "HF_USERNAME = \"Debk\"  # <-- IMPORTANT: SET YOUR HUGGING FACE USERNAME HERE\n",
    "NEW_MODEL_REPO_NAME = f\"{HF_USERNAME}/gemma-2b-finetuned-alpaca-hindi\"  # Updated for Gemma-2B\n",
    "FINAL_MODEL_DIR = \"./gemma-2b-hindi-final\"  # Updated for Gemma-2B\n",
    "BIAS_RESULTS_DIR = \"./bias_analysis_results\"\n",
    "\n",
    "# Language mapping for WEATHub dataset codes vs full names\n",
    "language_mapping = {\n",
    "    'english': 'en',\n",
    "    'hindi': 'hi', \n",
    "    'bengali': 'bn'\n",
    "}\n",
    "\n",
    "# Reverse mapping for storing full language names in results\n",
    "reverse_language_mapping = {v: k for k, v in language_mapping.items()}\n",
    "\n",
    "# =============================================================================\n",
    "# PLATFORM-SPECIFIC CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def setup_platform_environment(platform: str = \"local\"):\n",
    "    \"\"\"\n",
    "    Configures the environment based on the specified platform.\n",
    "    \"\"\"\n",
    "    print(f\"Setting up environment for: {platform.upper()}\")\n",
    "    project_path = \"./\"\n",
    "    results_path = BIAS_RESULTS_DIR\n",
    "    hf_cache_dir = \"./hf_cache/\"\n",
    "\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "\n",
    "    # Attempt to log in to Hugging Face if a token is available\n",
    "    hf_token = os.environ.get('HF_TOKEN')\n",
    "    if hf_token:\n",
    "        try:\n",
    "            login(hf_token, add_to_git_credential=True)\n",
    "            print(\"✅ Successfully logged in to Hugging Face!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to login to Hugging Face: {e}\")\n",
    "    else:\n",
    "        print(\"Hugging Face token not found in environment variables. Login manually if needed.\")\n",
    "\n",
    "    print(f\"Project path set to: {project_path}\")\n",
    "    print(f\"Bias analysis results will be saved to: {results_path}\")\n",
    "\n",
    "    return project_path, results_path, hf_cache_dir\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LAYER-WISE BIAS ANALYSIS COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "class LLMManager:\n",
    "    \"\"\"Manages the lifecycle of LLMs for analysis to optimize memory.\"\"\"\n",
    "    def __init__(self, cache_dir: str):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.current_model_id = None\n",
    "\n",
    "    def load_model(self, model_id: str, tokenizer_id: str, model_repo: str):\n",
    "        \"\"\"Load model and tokenizer for Gemma-2B.\"\"\"  # Updated comment\n",
    "        if self.current_model_id == model_id and self.model is not None:\n",
    "            print(f\"Model '{model_id}' already loaded.\")\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        print(f\"Loading model: {model_id} and tokenizer: {tokenizer_id}\")\n",
    "        load_path = model_id if model_repo == 'hf' else model_repo\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, cache_dir=self.cache_dir)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            # Configure 4-bit quantization\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "            # Load Gemma-2B model  # Updated comment\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                load_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                cache_dir=self.cache_dir\n",
    "            )\n",
    "            self.current_model_id = model_id\n",
    "            print(f\"Model '{model_id}' loaded successfully.\")\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load model '{model_id}'. Exception: {e}\")\n",
    "            print(\"Please ensure you have access to the Gemma model and are logged in to Hugging Face.\")  # Updated message\n",
    "            return None, None\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Unloads the model and clears GPU cache.\"\"\"\n",
    "        if self.model:\n",
    "            print(f\"Unloading model: {self.current_model_id}...\")\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            self.model, self.tokenizer, self.current_model_id = None, None, None\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Model unloaded and memory cleared.\")\n",
    "\n",
    "class WEATHubLoader:\n",
    "    \"\"\"Loads the WEATHub dataset and provides word lists.\"\"\"\n",
    "    def __init__(self, dataset_id: str, cache_dir: str = None):\n",
    "        print(f\"Loading WEATHub dataset from '{dataset_id}'...\")\n",
    "        try:\n",
    "            self.dataset = load_dataset(dataset_id, cache_dir=cache_dir)\n",
    "            print(\"WEATHub dataset loaded successfully.\")\n",
    "            self.split_mapping = {\n",
    "                'WEAT1': 'original_weat', 'WEAT2': 'original_weat', 'WEAT6': 'original_weat', 'WEAT7': 'original_weat', 'WEAT8': 'original_weat'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load WEATHub dataset. Exception: {e}\")\n",
    "            self.dataset = None\n",
    "\n",
    "    def get_word_lists(self, language_code: str, weat_category_id: str):\n",
    "        \"\"\"Retrieves target and attribute word lists.\"\"\"\n",
    "        if not self.dataset: return None\n",
    "        split_name = self.split_mapping.get(weat_category_id)\n",
    "        if not split_name:\n",
    "            print(f\"Warning: Category '{weat_category_id}' not found.\")\n",
    "            return None\n",
    "        try:\n",
    "            filtered = self.dataset[split_name].filter(lambda x: x['language'] == language_code and x['weat'] == weat_category_id)\n",
    "            if len(filtered) > 0:\n",
    "                return { 'targ1': filtered[0]['targ1.examples'], 'targ2': filtered[0]['targ2.examples'], 'attr1': filtered[0]['attr1.examples'], 'attr2': filtered[0]['attr2.examples'] }\n",
    "            else:\n",
    "                print(f\"Warning: No data for language '{language_code}' and category '{weat_category_id}'.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error filtering data for '{weat_category_id}' in language '{language_code}': {e}\")\n",
    "            return None\n",
    "\n",
    "class LayerEmbeddingExtractor:\n",
    "    \"\"\"Extracts hidden states from model layers.\"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embeddings(self, words: list, layer_idx: int):\n",
    "        \"\"\"Gets embeddings for a list of words at a specific layer.\"\"\"\n",
    "        all_embeddings = []\n",
    "        for word in words:\n",
    "            inputs = self.tokenizer(word, return_tensors=\"pt\", add_special_tokens=False).to(self.device)\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            # Use the hidden states from the specified layer\n",
    "            word_embedding = outputs.hidden_states[layer_idx][0].mean(dim=0).float().cpu().numpy()\n",
    "            all_embeddings.append(word_embedding)\n",
    "        return np.array(all_embeddings)\n",
    "\n",
    "class BiasQuantifier:\n",
    "    \"\"\"Calculates bias scores using WEAT effect size.\"\"\"\n",
    "    def _s(self, w, A, B):\n",
    "        mean_cos_A = np.mean([cosine_similarity([w], [a])[0][0] for a in A])\n",
    "        mean_cos_B = np.mean([cosine_similarity([w], [b])[0][0] for b in B])\n",
    "        return mean_cos_A - mean_cos_B\n",
    "\n",
    "    def weat_effect_size(self, T1_embeds, T2_embeds, A1_embeds, A2_embeds):\n",
    "        \"\"\"Calculates the WEAT effect size (d-score).\"\"\"\n",
    "        mean_T1 = np.mean([self._s(t, A1_embeds, A2_embeds) for t in T1_embeds])\n",
    "        mean_T2 = np.mean([self._s(t, A1_embeds, A2_embeds) for t in T2_embeds])\n",
    "        all_s = [self._s(t, A1_embeds, A2_embeds) for t in np.concatenate((T1_embeds, T2_embeds))]\n",
    "        std_dev = np.std(all_s, ddof=1)\n",
    "        return (mean_T1 - mean_T2) / std_dev if std_dev > 0 else 0\n",
    "\n",
    "def create_detailed_comment(base_comment: str, language: str = \"hindi\", dataset: str = \"alpaca\", model: str = \"gemma-2b\", mode: str = None):\n",
    "    \"\"\"Creates a detailed comment for logging purposes.\"\"\"\n",
    "    # Updated to use gemma-2b in comments\n",
    "    detailed_comment = f\"{base_comment} {language} finetune on {model}\"\n",
    "    return detailed_comment\n",
    "\n",
    "def execute_bias_analysis(model, tokenizer, results_path: str, hf_cache_dir: str, model_name: str, comments: str, languages: list, mode: str = None):\n",
    "    \"\"\"Runs the layer-wise bias analysis and saves the results.\"\"\"\n",
    "    weathub_loader = WEATHubLoader(dataset_id='iamshnoo/WEATHub', cache_dir=os.path.join(hf_cache_dir, \"datasets\"))\n",
    "    bias_quantifier = BiasQuantifier()\n",
    "    num_layers = len(model.model.layers)  # For Gemma models (same structure as Llama)\n",
    "    embedding_extractor = LayerEmbeddingExtractor(model, tokenizer)\n",
    "    all_results = []\n",
    "    weat_categories_to_test = ['WEAT1', 'WEAT2', 'WEAT6']\n",
    "    \n",
    "    # Updated to use gemma-2b in comments\n",
    "    detailed_comment = create_detailed_comment(comments, model=\"gemma-2b\", mode=mode)\n",
    "\n",
    "    for lang_full in languages:\n",
    "        # Convert full language name to code for WEATHub\n",
    "        lang_code = language_mapping.get(lang_full, lang_full)\n",
    "        for weat_cat in weat_categories_to_test:\n",
    "            print(f\"\\nProcessing: Lang='{lang_full}' (code: {lang_code}), Category='{weat_cat}'\")\n",
    "            word_lists = weathub_loader.get_word_lists(lang_code, weat_cat)\n",
    "            if not word_lists: continue\n",
    "            for layer_idx in tqdm(range(num_layers), desc=f\"Layer Analysis ({lang_full}/{weat_cat})\"):\n",
    "                t1_embeds = embedding_extractor.get_embeddings(word_lists['targ1'], layer_idx)\n",
    "                t2_embeds = embedding_extractor.get_embeddings(word_lists['targ2'], layer_idx)\n",
    "                a1_embeds = embedding_extractor.get_embeddings(word_lists['attr1'], layer_idx)\n",
    "                a2_embeds = embedding_extractor.get_embeddings(word_lists['attr2'], layer_idx)\n",
    "                weat_score = bias_quantifier.weat_effect_size(t1_embeds, t2_embeds, a1_embeds, a2_embeds)\n",
    "                # Store full language name in results\n",
    "                all_results.append({'model_id': model_name, 'language': lang_full, 'weat_category_id': weat_cat, 'layer_idx': layer_idx, 'weat_score': weat_score, 'comments': detailed_comment})\n",
    "\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        filename = f\"bias_results_{model_name.replace('/', '_')}_{detailed_comment.replace(' ', '_')}.csv\"\n",
    "        filepath = os.path.join(results_path, filename)\n",
    "        results_df.to_csv(filepath, index=False)\n",
    "        print(f\"Results successfully saved to: {filepath}\")\n",
    "    else:\n",
    "        print(\"No results were generated.\")\n",
    "    print(\"\\nAnalysis complete.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FINE-TUNING AND UPLOAD COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "def create_prompt(example):\n",
    "    \"\"\"Creates a formatted instruction prompt from a dataset example.\"\"\"\n",
    "    template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    # Add EOS token for proper training\n",
    "    return template.format(instruction=example[\"instruction\"], output=example['output'] + \"</s>\")\n",
    "\n",
    "class BiasAnalysisCallback(TrainerCallback):\n",
    "    \"\"\"A custom TrainerCallback that runs bias analysis at the end of each epoch.\"\"\"\n",
    "    def __init__(self, tokenizer, results_path, hf_cache_dir, model_name, mode):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.results_path = results_path\n",
    "        self.hf_cache_dir = hf_cache_dir\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch = int(state.epoch)\n",
    "        model = kwargs['model']\n",
    "        print(f\"\\n--- Running Bias Analysis for Epoch {epoch} ---\")\n",
    "        execute_bias_analysis(model, self.tokenizer, self.results_path, self.hf_cache_dir, self.model_name, f\"After epoch {epoch}\", ['hindi', 'english'], self.mode)\n",
    "        print(f\"--- Bias Analysis for Epoch {epoch} Completed ---\")\n",
    "\n",
    "def upload_to_hf(model_path, repo_name):\n",
    "    \"\"\"Uploads a model folder and associated artifacts to the Hugging Face Hub.\"\"\"\n",
    "    print(f\"Starting upload of '{model_path}' to '{repo_name}'...\")\n",
    "    \n",
    "    try:\n",
    "        user_info = whoami()\n",
    "        current_user = user_info.get('name')\n",
    "        print(f\"✅ Authenticated as: {current_user}\")\n",
    "        \n",
    "        expected_user = repo_name.split('/')[0]\n",
    "        if current_user != expected_user:\n",
    "            print(f\"⚠️ WARNING: Authenticated user '{current_user}' doesn't match repo owner '{expected_user}'\")\n",
    "            repo_name = f\"{current_user}/{repo_name.split('/')[1]}\"\n",
    "            print(f\"🔄 Using corrected repo name: {repo_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Authentication check failed: {e}. Please log in first.\")\n",
    "        return False\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model directory '{model_path}' not found!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        api = HfApi()\n",
    "        print(f\"📁 Creating repository: {repo_name}\")\n",
    "        api.create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)\n",
    "        \n",
    "        print(f\"📤 Uploading folder: {model_path}\")\n",
    "        api.upload_folder(\n",
    "            folder_path=model_path, \n",
    "            repo_id=repo_name, \n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"Upload fine-tuned Gemma-2B model for Hindi\"  # Updated message\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Upload completed successfully!\")\n",
    "        print(f\"🔗 View your model at: https://huggingface.co/{repo_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Upload failed with error: {e}\")\n",
    "        return False\n",
    "\n",
    "def merge_csv_files(results_path: str, model_name: str):\n",
    "    \"\"\"Merge all CSV files for this model into one consolidated file.\"\"\"\n",
    "    print(f\"\\n--- Merging CSV files for {model_name} ---\")\n",
    "    \n",
    "    # Find all CSV files for this model\n",
    "    model_clean = model_name.replace('/', '_')\n",
    "    pattern = os.path.join(results_path, f\"bias_results_{model_clean}_*.csv\")\n",
    "    csv_files = glob.glob(pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found for model {model_name}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to merge:\")\n",
    "    for file in csv_files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "    # Read and combine all CSV files\n",
    "    all_dataframes = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_dataframes.append(df)\n",
    "            print(f\"  ✅ Loaded {file} with {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    if all_dataframes:\n",
    "        # Merge all dataframes\n",
    "        merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        \n",
    "        # Sort by comments (to group before/after analyses together) and then by epoch\n",
    "        merged_df = merged_df.sort_values(['comments', 'language', 'weat_category_id', 'layer_idx'])\n",
    "        \n",
    "        # Save merged file\n",
    "        merged_filename = f\"bias_results_{model_clean}_merged_all_epochs.csv\"\n",
    "        merged_filepath = os.path.join(results_path, merged_filename)\n",
    "        merged_df.to_csv(merged_filepath, index=False)\n",
    "        \n",
    "        print(f\"✅ Merged CSV saved to: {merged_filepath}\")\n",
    "        print(f\"Total rows in merged file: {len(merged_df)}\")\n",
    "        \n",
    "        # Show summary\n",
    "        print(\"\\nSummary of merged data:\")\n",
    "        print(f\"Languages: {sorted(merged_df['language'].unique())}\")\n",
    "        print(f\"WEAT categories: {sorted(merged_df['weat_category_id'].unique())}\")\n",
    "        print(f\"Comments (analysis stages): {sorted(merged_df['comments'].unique())}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No valid CSV files could be loaded for merging.\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main function to orchestrate the fine-tuning and analysis process.\"\"\"\n",
    "    project_path, results_path, hf_cache_dir = setup_platform_environment(args.platform)\n",
    "    \n",
    "    repo_name_with_mode = f\"{NEW_MODEL_REPO_NAME}_{args.mode}\"\n",
    "\n",
    "    if args.action == 'upload':\n",
    "        if not os.path.exists(FINAL_MODEL_DIR):\n",
    "            print(f\"Error: Final model directory '{FINAL_MODEL_DIR}' not found. Please run training first.\")\n",
    "            return\n",
    "        upload_to_hf(FINAL_MODEL_DIR, repo_name_with_mode)\n",
    "        return\n",
    "\n",
    "    # --- Initial Bias Analysis (Before Fine-tuning) ---\n",
    "    print(\"\\n--- Running Initial Bias Analysis on Base Gemma-2B Model ---\")  # Updated message\n",
    "    llm_manager = LLMManager(cache_dir=hf_cache_dir)\n",
    "    base_model, base_tokenizer = llm_manager.load_model(BASE_MODEL_NAME, TOKENIZER_NAME, 'hf')\n",
    "    if base_model and base_tokenizer:\n",
    "        execute_bias_analysis(base_model, base_tokenizer, results_path, hf_cache_dir, BASE_MODEL_NAME, \"Before\", ['english'], args.mode)\n",
    "    llm_manager.unload_model()\n",
    "    print(\"--- Initial Bias Analysis Completed ---\")\n",
    "\n",
    "    # --- Fine-tuning ---\n",
    "    print(\"\\n--- Preparing for Fine-tuning Gemma-2B ---\")  # Updated message\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "    \n",
    "    # Load the Gemma-2B tokenizer  # Updated comment\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, cache_dir=hf_cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"Padding token not found. Setting pad_token to eos_token.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the Gemma-2B model for fine-tuning  # Updated comment\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=hf_cache_dir\n",
    "    )\n",
    "    \n",
    "    # Synchronize model embeddings with tokenizer if needed\n",
    "    print(f\"Original model vocab size: {model.config.vocab_size}, Tokenizer vocab size: {len(tokenizer)}\")\n",
    "    if model.config.vocab_size != len(tokenizer):\n",
    "        print(\"Resizing model token embeddings to match tokenizer...\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        print(f\"New model vocab size: {model.config.vocab_size}\")\n",
    "    \n",
    "    # Ensure model's pad_token_id is configured\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    dataset_with_prompt = dataset.map(lambda example: {\"text\": create_prompt(example)})\n",
    "    tokenized_dataset = dataset_with_prompt.map(lambda ex: tokenizer(ex[\"text\"], truncation=True, max_length=512), batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    num_train_epochs = 1 if args.mode == 'test' else 5 # Use 5 epochs for full mode\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./gemma-2b-hindi-tuned\",  # Updated directory name\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        max_steps=-1,\n",
    "        bf16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    if args.mode == 'test':\n",
    "        shuffled_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "        # Use a small subset for testing\n",
    "        train_dataset = shuffled_dataset.select(range(100))\n",
    "        eval_dataset = shuffled_dataset.select(range(100, 120))\n",
    "    else:\n",
    "        split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        train_dataset, eval_dataset = split_dataset['train'], split_dataset['test']\n",
    "\n",
    "    bias_analysis_callback = BiasAnalysisCallback(tokenizer, results_path, hf_cache_dir, BASE_MODEL_NAME, args.mode)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=train_dataset, \n",
    "        eval_dataset=eval_dataset, \n",
    "        data_collator=data_collator, \n",
    "        tokenizer=tokenizer, \n",
    "        callbacks=[bias_analysis_callback]\n",
    "    )\n",
    "\n",
    "    print(f\"--- Starting Fine-tuning (Mode: {args.mode}) ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Fine-tuning Completed ---\")\n",
    "\n",
    "    print(f\"Saving final model to {FINAL_MODEL_DIR}\")\n",
    "    trainer.save_model(FINAL_MODEL_DIR)\n",
    "    \n",
    "    # --- Merge all CSV files ---\n",
    "    merge_csv_files(results_path, BASE_MODEL_NAME)\n",
    "    \n",
    "    print(\"\\n--- Starting Automatic Upload to HuggingFace Hub ---\")\n",
    "    if os.path.exists(FINAL_MODEL_DIR):\n",
    "        upload_to_hf(FINAL_MODEL_DIR, repo_name_with_mode)\n",
    "        print(\"--- Upload to HuggingFace Hub Completed ---\")\n",
    "    else:\n",
    "        print(f\"Error: Final model directory '{FINAL_MODEL_DIR}' not found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    is_notebook = 'google.colab' in sys.modules or 'ipykernel' in sys.modules\n",
    "\n",
    "    if is_notebook:\n",
    "        print(\"Running in a notebook environment. Setting arguments manually.\")\n",
    "        # Manually set args for notebook execution\n",
    "        args = argparse.Namespace(action=\"train\", mode=\"full\", platform=\"local\")\n",
    "        main(args)\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description=\"Fine-tune and analyze Gemma-2B.\")  # Updated description\n",
    "        parser.add_argument(\"--action\", type=str, default=\"train\", choices=[\"train\", \"upload\"], help=\"Action to perform.\")\n",
    "        parser.add_argument(\"--mode\", type=str, default=\"test\", choices=[\"test\", \"full\"], help=\"Training mode (test uses a small subset).\")\n",
    "        parser.add_argument(\"--platform\", type=str, default=\"local\", choices=[\"colab\", \"digitalocean\", \"local\"], help=\"Execution platform.\")\n",
    "        parsed_args = parser.parse_args()\n",
    "        main(parsed_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55757e8",
   "metadata": {},
   "source": [
    "For Execution on Bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418805c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script fine-tunes a Gemma-2B model that was previously fine-tuned on Hindi data \n",
    "and performs layer-wise bias analysis before and after training on Bengali data.\n",
    "\n",
    "Workflow:\n",
    "1.  **Initial Bias Analysis**: Performs a layer-wise bias analysis on the base\n",
    "    model (`Debk/gemma-2b-finetuned-alpaca-hindi_full`) using Hindi and English datasets before any Bengali fine-tuning.\n",
    "2.  **Fine-tuning**: Fine-tunes the `Debk/gemma-2b-finetuned-alpaca-hindi_full` model on a Bengali dataset\n",
    "    (`iamshnoo/alpaca-cleaned-bengali`).\n",
    "3.  **Post-Epoch Bias Analysis**: After each fine-tuning epoch, the script runs the\n",
    "    layer-wise bias analysis on the model for Hindi, English, and Bengali to track how\n",
    "    bias evolves.\n",
    "4.  **Results**: All bias analysis results are saved to CSV files for further examination.\n",
    "5.  **Upload**: After training, the script can upload the final model, tokenizer,\n",
    "    and required custom code to the Hugging Face Hub.\n",
    "\n",
    "This script can be executed with command-line arguments to specify the action\n",
    "('train' or 'upload'), training mode ('test' or 'full'), and the platform\n",
    "('colab', 'digitalocean', or 'local').\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import csv\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, login, whoami\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# =============================================================================\n",
    "BASE_MODEL_NAME = \"Debk/gemma-2b-finetuned-alpaca-hindi_full\"  # Fine-tuned Hindi model to build upon\n",
    "TOKENIZER_NAME = \"Debk/gemma-2b-finetuned-alpaca-hindi_full\"  # Use same tokenizer as model\n",
    "DATASET_NAME = \"iamshnoo/alpaca-cleaned-bengali\"  # Bengali dataset for further finetuning\n",
    "HF_USERNAME = \"Debk\"  # <-- IMPORTANT: SET YOUR HUGGING FACE USERNAME HERE\n",
    "NEW_MODEL_REPO_NAME = f\"{HF_USERNAME}/gemma-2b-finetuned-alpaca-hindi-bengali\"  # Updated for Hindi-Bengali model\n",
    "FINAL_MODEL_DIR = \"./gemma-2b-hindi-bengali-final\"  # Updated for Hindi-Bengali model\n",
    "BIAS_RESULTS_DIR = \"./bias_analysis_results\"\n",
    "\n",
    "# Language mapping for WEATHub dataset codes vs full names\n",
    "language_mapping = {\n",
    "    'english': 'en',\n",
    "    'hindi': 'hi', \n",
    "    'bengali': 'bn'\n",
    "}\n",
    "\n",
    "# Reverse mapping for storing full language names in results\n",
    "reverse_language_mapping = {v: k for k, v in language_mapping.items()}\n",
    "\n",
    "# =============================================================================\n",
    "# PLATFORM-SPECIFIC CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def setup_platform_environment(platform: str = \"local\"):\n",
    "    \"\"\"\n",
    "    Configures the environment based on the specified platform.\n",
    "    \"\"\"\n",
    "    print(f\"Setting up environment for: {platform.upper()}\")\n",
    "    project_path = \"./\"\n",
    "    results_path = BIAS_RESULTS_DIR\n",
    "    hf_cache_dir = \"./hf_cache/\"\n",
    "\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "\n",
    "    # Attempt to log in to Hugging Face if a token is available\n",
    "    hf_token = os.environ.get('HF_TOKEN')\n",
    "    if hf_token:\n",
    "        try:\n",
    "            login(hf_token, add_to_git_credential=True)\n",
    "            print(\"✅ Successfully logged in to Hugging Face!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to login to Hugging Face: {e}\")\n",
    "    else:\n",
    "        print(\"Hugging Face token not found in environment variables. Login manually if needed.\")\n",
    "\n",
    "    print(f\"Project path set to: {project_path}\")\n",
    "    print(f\"Bias analysis results will be saved to: {results_path}\")\n",
    "\n",
    "    return project_path, results_path, hf_cache_dir\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LAYER-WISE BIAS ANALYSIS COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "class LLMManager:\n",
    "    \"\"\"Manages the lifecycle of LLMs for analysis to optimize memory.\"\"\"\n",
    "    def __init__(self, cache_dir: str):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.current_model_id = None\n",
    "\n",
    "    def load_model(self, model_id: str, tokenizer_id: str, model_repo: str):\n",
    "        \"\"\"Load model and tokenizer for fine-tuned Gemma-2B (Hindi).\"\"\"  # Updated comment\n",
    "        if self.current_model_id == model_id and self.model is not None:\n",
    "            print(f\"Model '{model_id}' already loaded.\")\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        print(f\"Loading model: {model_id} and tokenizer: {tokenizer_id}\")\n",
    "        load_path = model_id if model_repo == 'hf' else model_repo\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, cache_dir=self.cache_dir)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            # Configure 4-bit quantization\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "            # Load fine-tuned Gemma-2B model (Hindi)  # Updated comment\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                load_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                cache_dir=self.cache_dir\n",
    "            )\n",
    "            self.current_model_id = model_id\n",
    "            print(f\"Model '{model_id}' loaded successfully.\")\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load model '{model_id}'. Exception: {e}\")\n",
    "            print(\"Please ensure you have access to the fine-tuned Gemma model and are logged in to Hugging Face.\")  # Updated message\n",
    "            return None, None\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Unloads the model and clears GPU cache.\"\"\"\n",
    "        if self.model:\n",
    "            print(f\"Unloading model: {self.current_model_id}...\")\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            self.model, self.tokenizer, self.current_model_id = None, None, None\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Model unloaded and memory cleared.\")\n",
    "\n",
    "class WEATHubLoader:\n",
    "    \"\"\"Loads the WEATHub dataset and provides word lists.\"\"\"\n",
    "    def __init__(self, dataset_id: str, cache_dir: str = None):\n",
    "        print(f\"Loading WEATHub dataset from '{dataset_id}'...\")\n",
    "        try:\n",
    "            self.dataset = load_dataset(dataset_id, cache_dir=cache_dir)\n",
    "            print(\"WEATHub dataset loaded successfully.\")\n",
    "            self.split_mapping = {\n",
    "                'WEAT1': 'original_weat', 'WEAT2': 'original_weat', 'WEAT6': 'original_weat', 'WEAT7': 'original_weat', 'WEAT8': 'original_weat'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load WEATHub dataset. Exception: {e}\")\n",
    "            self.dataset = None\n",
    "\n",
    "    def get_word_lists(self, language_code: str, weat_category_id: str):\n",
    "        \"\"\"Retrieves target and attribute word lists.\"\"\"\n",
    "        if not self.dataset: return None\n",
    "        split_name = self.split_mapping.get(weat_category_id)\n",
    "        if not split_name:\n",
    "            print(f\"Warning: Category '{weat_category_id}' not found.\")\n",
    "            return None\n",
    "        try:\n",
    "            filtered = self.dataset[split_name].filter(lambda x: x['language'] == language_code and x['weat'] == weat_category_id)\n",
    "            if len(filtered) > 0:\n",
    "                return { 'targ1': filtered[0]['targ1.examples'], 'targ2': filtered[0]['targ2.examples'], 'attr1': filtered[0]['attr1.examples'], 'attr2': filtered[0]['attr2.examples'] }\n",
    "            else:\n",
    "                print(f\"Warning: No data for language '{language_code}' and category '{weat_category_id}'.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error filtering data for '{weat_category_id}' in language '{language_code}': {e}\")\n",
    "            return None\n",
    "\n",
    "class LayerEmbeddingExtractor:\n",
    "    \"\"\"Extracts hidden states from model layers.\"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embeddings(self, words: list, layer_idx: int):\n",
    "        \"\"\"Gets embeddings for a list of words at a specific layer.\"\"\"\n",
    "        all_embeddings = []\n",
    "        for word in words:\n",
    "            inputs = self.tokenizer(word, return_tensors=\"pt\", add_special_tokens=False).to(self.device)\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            # Use the hidden states from the specified layer\n",
    "            word_embedding = outputs.hidden_states[layer_idx][0].mean(dim=0).float().cpu().numpy()\n",
    "            all_embeddings.append(word_embedding)\n",
    "        return np.array(all_embeddings)\n",
    "\n",
    "class BiasQuantifier:\n",
    "    \"\"\"Calculates bias scores using WEAT effect size.\"\"\"\n",
    "    def _s(self, w, A, B):\n",
    "        mean_cos_A = np.mean([cosine_similarity([w], [a])[0][0] for a in A])\n",
    "        mean_cos_B = np.mean([cosine_similarity([w], [b])[0][0] for b in B])\n",
    "        return mean_cos_A - mean_cos_B\n",
    "\n",
    "    def weat_effect_size(self, T1_embeds, T2_embeds, A1_embeds, A2_embeds):\n",
    "        \"\"\"Calculates the WEAT effect size (d-score).\"\"\"\n",
    "        mean_T1 = np.mean([self._s(t, A1_embeds, A2_embeds) for t in T1_embeds])\n",
    "        mean_T2 = np.mean([self._s(t, A1_embeds, A2_embeds) for t in T2_embeds])\n",
    "        all_s = [self._s(t, A1_embeds, A2_embeds) for t in np.concatenate((T1_embeds, T2_embeds))]\n",
    "        std_dev = np.std(all_s, ddof=1)\n",
    "        return (mean_T1 - mean_T2) / std_dev if std_dev > 0 else 0\n",
    "\n",
    "def create_detailed_comment(base_comment: str, language: str = \"bengali\", dataset: str = \"alpaca\", model: str = \"gemma-2b-finetuned-alpaca-hindi_full\", mode: str = None):\n",
    "    \"\"\"Creates a detailed comment for logging purposes.\"\"\"\n",
    "    # Updated to reflect Bengali finetuning on Hindi-finetuned model\n",
    "    detailed_comment = f\"{base_comment} {language} finetune on {model}\"\n",
    "    return detailed_comment\n",
    "\n",
    "def execute_bias_analysis(model, tokenizer, results_path: str, hf_cache_dir: str, model_name: str, comments: str, languages: list, mode: str = None):\n",
    "    \"\"\"Runs the layer-wise bias analysis and saves the results.\"\"\"\n",
    "    weathub_loader = WEATHubLoader(dataset_id='iamshnoo/WEATHub', cache_dir=os.path.join(hf_cache_dir, \"datasets\"))\n",
    "    bias_quantifier = BiasQuantifier()\n",
    "    num_layers = len(model.model.layers)  # For Gemma models (same structure as Llama)\n",
    "    embedding_extractor = LayerEmbeddingExtractor(model, tokenizer)\n",
    "    all_results = []\n",
    "    weat_categories_to_test = ['WEAT1', 'WEAT2', 'WEAT6']\n",
    "    \n",
    "    # Updated to reflect Bengali finetuning on Hindi-finetuned model\n",
    "    detailed_comment = create_detailed_comment(comments, model=\"gemma-2b-finetuned-alpaca-hindi_full\", mode=mode)\n",
    "\n",
    "    for lang_full in languages:\n",
    "        # Convert full language name to code for WEATHub\n",
    "        lang_code = language_mapping.get(lang_full, lang_full)\n",
    "        for weat_cat in weat_categories_to_test:\n",
    "            print(f\"\\nProcessing: Lang='{lang_full}' (code: {lang_code}), Category='{weat_cat}'\")\n",
    "            word_lists = weathub_loader.get_word_lists(lang_code, weat_cat)\n",
    "            if not word_lists: continue\n",
    "            for layer_idx in tqdm(range(num_layers), desc=f\"Layer Analysis ({lang_full}/{weat_cat})\"):\n",
    "                t1_embeds = embedding_extractor.get_embeddings(word_lists['targ1'], layer_idx)\n",
    "                t2_embeds = embedding_extractor.get_embeddings(word_lists['targ2'], layer_idx)\n",
    "                a1_embeds = embedding_extractor.get_embeddings(word_lists['attr1'], layer_idx)\n",
    "                a2_embeds = embedding_extractor.get_embeddings(word_lists['attr2'], layer_idx)\n",
    "                weat_score = bias_quantifier.weat_effect_size(t1_embeds, t2_embeds, a1_embeds, a2_embeds)\n",
    "                # Store full language name in results\n",
    "                all_results.append({'model_id': model_name, 'language': lang_full, 'weat_category_id': weat_cat, 'layer_idx': layer_idx, 'weat_score': weat_score, 'comments': detailed_comment})\n",
    "\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        filename = f\"bias_results_{model_name.replace('/', '_')}_{detailed_comment.replace(' ', '_')}.csv\"\n",
    "        filepath = os.path.join(results_path, filename)\n",
    "        results_df.to_csv(filepath, index=False)\n",
    "        print(f\"Results successfully saved to: {filepath}\")\n",
    "    else:\n",
    "        print(\"No results were generated.\")\n",
    "    print(\"\\nAnalysis complete.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FINE-TUNING AND UPLOAD COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "def create_prompt(example):\n",
    "    \"\"\"Creates a formatted instruction prompt from a dataset example.\"\"\"\n",
    "    template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    # Add EOS token for proper training\n",
    "    return template.format(instruction=example[\"instruction\"], output=example['output'] + \"</s>\")\n",
    "\n",
    "class BiasAnalysisCallback(TrainerCallback):\n",
    "    \"\"\"A custom TrainerCallback that runs bias analysis at the end of each epoch.\"\"\"\n",
    "    def __init__(self, tokenizer, results_path, hf_cache_dir, model_name, mode):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.results_path = results_path\n",
    "        self.hf_cache_dir = hf_cache_dir\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch = int(state.epoch)\n",
    "        model = kwargs['model']\n",
    "        print(f\"\\n--- Running Bias Analysis for Epoch {epoch} ---\")\n",
    "        execute_bias_analysis(model, self.tokenizer, self.results_path, self.hf_cache_dir, self.model_name, f\"After epoch {epoch}\", ['hindi', 'english', 'bengali'], self.mode)\n",
    "        print(f\"--- Bias Analysis for Epoch {epoch} Completed ---\")\n",
    "\n",
    "def upload_to_hf(model_path, repo_name):\n",
    "    \"\"\"Uploads a model folder and associated artifacts to the Hugging Face Hub.\"\"\"\n",
    "    print(f\"Starting upload of '{model_path}' to '{repo_name}'...\")\n",
    "    \n",
    "    try:\n",
    "        user_info = whoami()\n",
    "        current_user = user_info.get('name')\n",
    "        print(f\"✅ Authenticated as: {current_user}\")\n",
    "        \n",
    "        expected_user = repo_name.split('/')[0]\n",
    "        if current_user != expected_user:\n",
    "            print(f\"⚠️ WARNING: Authenticated user '{current_user}' doesn't match repo owner '{expected_user}'\")\n",
    "            repo_name = f\"{current_user}/{repo_name.split('/')[1]}\"\n",
    "            print(f\"🔄 Using corrected repo name: {repo_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Authentication check failed: {e}. Please log in first.\")\n",
    "        return False\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model directory '{model_path}' not found!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        api = HfApi()\n",
    "        print(f\"📁 Creating repository: {repo_name}\")\n",
    "        api.create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)\n",
    "        \n",
    "        print(f\"📤 Uploading folder: {model_path}\")\n",
    "        api.upload_folder(\n",
    "            folder_path=model_path, \n",
    "            repo_id=repo_name, \n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"Upload fine-tuned Gemma-2B model for Hindi-Bengali\"  # Updated message\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Upload completed successfully!\")\n",
    "        print(f\"🔗 View your model at: https://huggingface.co/{repo_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Upload failed with error: {e}\")\n",
    "        return False\n",
    "\n",
    "def merge_csv_files(results_path: str, model_name: str):\n",
    "    \"\"\"Merge all CSV files for this model into one consolidated file.\"\"\"\n",
    "    print(f\"\\n--- Merging CSV files for {model_name} ---\")\n",
    "    \n",
    "    # Find all CSV files for this model\n",
    "    model_clean = model_name.replace('/', '_')\n",
    "    pattern = os.path.join(results_path, f\"bias_results_{model_clean}_*.csv\")\n",
    "    csv_files = glob.glob(pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found for model {model_name}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to merge:\")\n",
    "    for file in csv_files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "    # Read and combine all CSV files\n",
    "    all_dataframes = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_dataframes.append(df)\n",
    "            print(f\"  ✅ Loaded {file} with {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    if all_dataframes:\n",
    "        # Merge all dataframes\n",
    "        merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        \n",
    "        # Sort by comments (to group before/after analyses together) and then by epoch\n",
    "        merged_df = merged_df.sort_values(['comments', 'language', 'weat_category_id', 'layer_idx'])\n",
    "        \n",
    "        # Save merged file\n",
    "        merged_filename = f\"bias_results_{model_clean}_merged_all_epochs.csv\"\n",
    "        merged_filepath = os.path.join(results_path, merged_filename)\n",
    "        merged_df.to_csv(merged_filepath, index=False)\n",
    "        \n",
    "        print(f\"✅ Merged CSV saved to: {merged_filepath}\")\n",
    "        print(f\"Total rows in merged file: {len(merged_df)}\")\n",
    "        \n",
    "        # Show summary\n",
    "        print(\"\\nSummary of merged data:\")\n",
    "        print(f\"Languages: {sorted(merged_df['language'].unique())}\")\n",
    "        print(f\"WEAT categories: {sorted(merged_df['weat_category_id'].unique())}\")\n",
    "        print(f\"Comments (analysis stages): {sorted(merged_df['comments'].unique())}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No valid CSV files could be loaded for merging.\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main function to orchestrate the fine-tuning and analysis process.\"\"\"\n",
    "    project_path, results_path, hf_cache_dir = setup_platform_environment(args.platform)\n",
    "    \n",
    "    repo_name_with_mode = f\"{NEW_MODEL_REPO_NAME}_{args.mode}\"\n",
    "\n",
    "    if args.action == 'upload':\n",
    "        if not os.path.exists(FINAL_MODEL_DIR):\n",
    "            print(f\"Error: Final model directory '{FINAL_MODEL_DIR}' not found. Please run training first.\")\n",
    "            return\n",
    "        upload_to_hf(FINAL_MODEL_DIR, repo_name_with_mode)\n",
    "        return\n",
    "\n",
    "    # --- Initial Bias Analysis (Before Fine-tuning) ---\n",
    "    print(\"\\n--- Running Initial Bias Analysis on Hindi-finetuned Gemma-2B Model ---\")  # Updated message\n",
    "    llm_manager = LLMManager(cache_dir=hf_cache_dir)\n",
    "    base_model, base_tokenizer = llm_manager.load_model(BASE_MODEL_NAME, TOKENIZER_NAME, 'hf')\n",
    "    if base_model and base_tokenizer:\n",
    "        execute_bias_analysis(base_model, base_tokenizer, results_path, hf_cache_dir, BASE_MODEL_NAME, \"Before\", ['hindi', 'english'], args.mode)\n",
    "    llm_manager.unload_model()\n",
    "    print(\"--- Initial Bias Analysis Completed ---\")\n",
    "\n",
    "    # --- Fine-tuning ---\n",
    "    print(\"\\n--- Preparing for Bengali Fine-tuning on Hindi-finetuned Gemma-2B ---\")  # Updated message\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "    \n",
    "    # Load the fine-tuned Gemma-2B tokenizer  # Updated comment\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, cache_dir=hf_cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"Padding token not found. Setting pad_token to eos_token.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the Hindi-finetuned Gemma-2B model for Bengali fine-tuning  # Updated comment\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=hf_cache_dir\n",
    "    )\n",
    "    \n",
    "    # Synchronize model embeddings with tokenizer if needed\n",
    "    print(f\"Original model vocab size: {model.config.vocab_size}, Tokenizer vocab size: {len(tokenizer)}\")\n",
    "    if model.config.vocab_size != len(tokenizer):\n",
    "        print(\"Resizing model token embeddings to match tokenizer...\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        print(f\"New model vocab size: {model.config.vocab_size}\")\n",
    "    \n",
    "    # Ensure model's pad_token_id is configured\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    dataset_with_prompt = dataset.map(lambda example: {\"text\": create_prompt(example)})\n",
    "    tokenized_dataset = dataset_with_prompt.map(lambda ex: tokenizer(ex[\"text\"], truncation=True, max_length=512), batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    num_train_epochs = 1 if args.mode == 'test' else 5 # Use 5 epochs for full mode\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./gemma-2b-hindi-bengali-tuned\",  # Updated directory name\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        max_steps=-1,\n",
    "        bf16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    if args.mode == 'test':\n",
    "        shuffled_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "        # Use a small subset for testing\n",
    "        train_dataset = shuffled_dataset.select(range(100))\n",
    "        eval_dataset = shuffled_dataset.select(range(100, 120))\n",
    "    else:\n",
    "        split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        train_dataset, eval_dataset = split_dataset['train'], split_dataset['test']\n",
    "\n",
    "    bias_analysis_callback = BiasAnalysisCallback(tokenizer, results_path, hf_cache_dir, BASE_MODEL_NAME, args.mode)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=train_dataset, \n",
    "        eval_dataset=eval_dataset, \n",
    "        data_collator=data_collator, \n",
    "        tokenizer=tokenizer, \n",
    "        callbacks=[bias_analysis_callback]\n",
    "    )\n",
    "\n",
    "    print(f\"--- Starting Fine-tuning (Mode: {args.mode}) ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Fine-tuning Completed ---\")\n",
    "\n",
    "    print(f\"Saving final model to {FINAL_MODEL_DIR}\")\n",
    "    trainer.save_model(FINAL_MODEL_DIR)\n",
    "    \n",
    "    # --- Merge all CSV files ---\n",
    "    merge_csv_files(results_path, BASE_MODEL_NAME)\n",
    "    \n",
    "    print(\"\\n--- Starting Automatic Upload to HuggingFace Hub ---\")\n",
    "    if os.path.exists(FINAL_MODEL_DIR):\n",
    "        upload_to_hf(FINAL_MODEL_DIR, repo_name_with_mode)\n",
    "        print(\"--- Upload to HuggingFace Hub Completed ---\")\n",
    "    else:\n",
    "        print(f\"Error: Final model directory '{FINAL_MODEL_DIR}' not found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    is_notebook = 'google.colab' in sys.modules or 'ipykernel' in sys.modules\n",
    "\n",
    "    if is_notebook:\n",
    "        print(\"Running in a notebook environment. Setting arguments manually.\")\n",
    "        # Manually set args for notebook execution\n",
    "        args = argparse.Namespace(action=\"train\", mode=\"full\", platform=\"local\")\n",
    "        main(args)\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description=\"Fine-tune Hindi-finetuned Gemma-2B on Bengali data.\")  # Updated description\n",
    "        parser.add_argument(\"--action\", type=str, default=\"train\", choices=[\"train\", \"upload\"], help=\"Action to perform.\")\n",
    "        parser.add_argument(\"--mode\", type=str, default=\"test\", choices=[\"test\", \"full\"], help=\"Training mode (test uses a small subset).\")\n",
    "        parser.add_argument(\"--platform\", type=str, default=\"local\", choices=[\"colab\", \"digitalocean\", \"local\"], help=\"Execution platform.\")\n",
    "        parsed_args = parser.parse_args()\n",
    "        main(parsed_args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
