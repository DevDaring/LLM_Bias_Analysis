{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2049479",
   "metadata": {},
   "source": [
    "Full Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a8c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script merges the functionalities of fine-tuning a language model and performing\n",
    "layer-wise bias analysis, including the final step of uploading the model to the\n",
    "Hugging Face Hub. It is designed to be run on platforms like Google Colab or\n",
    "DigitalOcean.\n",
    "\n",
    "Workflow:\n",
    "1.  **Initial Bias Analysis**: Performs a layer-wise bias analysis on the base\n",
    "    model (`HuggingFaceTB/SmolLM2-135M`) using an English dataset before any fine-tuning.\n",
    "2.  **Fine-tuning**: Fine-tunes the `HuggingFaceTB/SmolLM2-135M` model on a Hindi dataset\n",
    "    (`iamshnoo/alpaca-cleaned-hindi`).\n",
    "3.  **Post-Epoch Bias Analysis**: After each fine-tuning epoch, the script runs the\n",
    "    layer-wise bias analysis on the model for both English and Hindi to track how\n",
    "    bias evolves.\n",
    "4.  **Results**: All bias analysis results are saved to CSV files for further examination.\n",
    "5.  **Upload**: After training, the script can be run again with the 'upload' action\n",
    "    to push the final model, tokenizer, and training artifacts to the Hugging Face Hub.\n",
    "\n",
    "This script can be executed with command-line arguments to specify the action\n",
    "('train' or 'upload'), training mode ('test' or 'full'), and the platform\n",
    "('colab' or 'local').\n",
    "\"\"\"\n",
    "\n",
    "# In a notebook environment (like Colab), run this cell first to install dependencies.\n",
    "# !pip install -q transformers>=4.32.0 datasets torch pandas numpy scikit-learn accelerate bitsandbytes\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, login\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# =============================================================================\n",
    "BASE_MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "DATASET_NAME = \"iamshnoo/alpaca-cleaned-hindi\"\n",
    "HF_USERNAME = \"DebK\"  # <-- IMPORTANT: SET YOUR USERNAME HERE\n",
    "NEW_MODEL_REPO_NAME = f\"{HF_USERNAME}/SmolLM2-135M-finetuned-alpaca-hindi\"\n",
    "FINAL_MODEL_DIR = \"./SmolLM2-135M-hindi-final\"\n",
    "BIAS_RESULTS_DIR = \"./bias_analysis_results\"\n",
    "\n",
    "# =============================================================================\n",
    "# PLATFORM-SPECIFIC CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def setup_platform_environment(platform: str = \"local\"):\n",
    "    \"\"\"\n",
    "    Configures the environment based on the specified platform.\n",
    "\n",
    "    Args:\n",
    "        platform (str): The target platform, e.g., 'colab', 'digitalocean', 'local'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (project_path, results_path, hf_cache_dir).\n",
    "    \"\"\"\n",
    "    print(f\"Setting up environment for: {platform.upper()}\")\n",
    "\n",
    "    if platform == \"colab\":\n",
    "        try:\n",
    "            from google.colab import drive, userdata\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive mounted successfully.\")\n",
    "\n",
    "            project_path = \"/content/drive/MyDrive/Mult_LLM_Bias/\"\n",
    "            results_path = os.path.join(project_path, \"Results/Experiment1/\")\n",
    "            hf_cache_dir = os.path.join(project_path, \"hf_cache/\")\n",
    "\n",
    "            print(\"Attempting to log in to Hugging Face...\")\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if HF_TOKEN:\n",
    "                login(HF_TOKEN, add_to_git_credential=True)\n",
    "                print(\"Successfully logged in to Hugging Face!\")\n",
    "            else:\n",
    "                print(\"Hugging Face token not found in Colab secrets.\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"Could not import Google Colab libraries. Defaulting to local setup.\")\n",
    "            return setup_platform_environment(\"local\")\n",
    "\n",
    "    elif platform == \"digitalocean\":\n",
    "        print(\"Setting up DigitalOcean environment...\")\n",
    "        \n",
    "        # Set paths for DigitalOcean - using root directory as base\n",
    "        project_path = \"/root/Mult_LLM_Bias/\"\n",
    "        results_path = os.path.join(project_path, \"Results/Experiment1/\")\n",
    "        hf_cache_dir = os.path.join(project_path, \"hf_cache/\")\n",
    "        \n",
    "        # Check if block storage volume is available (optional)\n",
    "        volume_path = \"/mnt/volume_nyc1_01/\"\n",
    "        if os.path.exists(volume_path):\n",
    "            print(\"DigitalOcean volume detected. Using volume storage...\")\n",
    "            project_path = os.path.join(volume_path, \"Mult_LLM_Bias/\")\n",
    "            results_path = os.path.join(project_path, \"Results/Experiment1/\")\n",
    "            hf_cache_dir = os.path.join(project_path, \"hf_cache/\")\n",
    "        else:\n",
    "            print(\"Using root directory storage...\")\n",
    "        \n",
    "        # Set up Hugging Face authentication for DigitalOcean\n",
    "        print(\"Setting up Hugging Face authentication...\")\n",
    "        \n",
    "        # First try environment variable\n",
    "        hf_token = os.environ.get('HF_TOKEN')\n",
    "        if not hf_token:\n",
    "            # Set your token\n",
    "            hf_token = \"hf_secret\"\n",
    "            # Set it as environment variable for this session\n",
    "            os.environ['HF_TOKEN'] = hf_token\n",
    "        \n",
    "        try:\n",
    "            login(hf_token, add_to_git_credential=True)\n",
    "            print(\"✅ Successfully logged in to Hugging Face!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to login to Hugging Face: {e}\")\n",
    "            print(\"Please check your token and internet connection.\")\n",
    "        \n",
    "        # Optimize for DigitalOcean environment\n",
    "        os.environ['TRANSFORMERS_CACHE'] = hf_cache_dir\n",
    "        os.environ['HF_HOME'] = hf_cache_dir\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU if available\n",
    "        \n",
    "        print(\"DigitalOcean environment configured successfully.\")\n",
    "\n",
    "    else: # Default to local setup\n",
    "        project_path = \"./\"\n",
    "        results_path = BIAS_RESULTS_DIR\n",
    "        hf_cache_dir = \"./hf_cache/\"\n",
    "\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Project path set to: {project_path}\")\n",
    "    print(f\"Bias analysis results will be saved to: {results_path}\")\n",
    "\n",
    "    return project_path, results_path, hf_cache_dir\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LAYER-WISE BIAS ANALYSIS COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "class LLMManager:\n",
    "    \"\"\"Manages the lifecycle of LLMs to optimize memory usage.\"\"\"\n",
    "    def __init__(self, cache_dir: str):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.current_model_id = None\n",
    "\n",
    "    def load_model(self, model_id: str, model_repo: str):\n",
    "        \"\"\"Load model and tokenizer from either Hugging Face or a local path.\"\"\"\n",
    "        if self.current_model_id == model_id and self.model is not None:\n",
    "            print(f\"Model '{model_id}' already loaded.\")\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        print(f\"Loading model: {model_id} from {model_repo}\")\n",
    "        load_path = model_id if model_repo == 'hf' else model_repo\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(load_path, cache_dir=self.cache_dir)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            #Update Config\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "             )\n",
    "\n",
    "            # quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                load_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                cache_dir=self.cache_dir\n",
    "            )\n",
    "            self.current_model_id = model_id\n",
    "            print(f\"Model '{model_id}' loaded successfully.\")\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load model '{model_id}'. Exception: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Unloads the model and clears GPU cache.\"\"\"\n",
    "        if self.model:\n",
    "            print(f\"Unloading model: {self.current_model_id}...\")\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            self.model, self.tokenizer, self.current_model_id = None, None, None\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Model unloaded and memory cleared.\")\n",
    "\n",
    "class WEATHubLoader:\n",
    "    \"\"\"Loads the WEATHub dataset and provides word lists.\"\"\"\n",
    "    def __init__(self, dataset_id: str, cache_dir: str = None):\n",
    "        print(f\"Loading WEATHub dataset from '{dataset_id}'...\")\n",
    "        try:\n",
    "            self.dataset = load_dataset(dataset_id, cache_dir=cache_dir)\n",
    "            print(\"WEATHub dataset loaded successfully.\")\n",
    "            self.split_mapping = {\n",
    "                'WEAT1': 'original_weat', 'WEAT2': 'original_weat', 'WEAT6': 'original_weat', 'WEAT7': 'original_weat', 'WEAT8': 'original_weat'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load WEATHub dataset. Exception: {e}\")\n",
    "            self.dataset = None\n",
    "\n",
    "    def get_word_lists(self, language_code: str, weat_category_id: str):\n",
    "        \"\"\"Retrieves target and attribute word lists.\"\"\"\n",
    "        if not self.dataset: return None\n",
    "        split_name = self.split_mapping.get(weat_category_id)\n",
    "        if not split_name:\n",
    "            print(f\"Warning: Category '{weat_category_id}' not found.\")\n",
    "            return None\n",
    "        try:\n",
    "            filtered = self.dataset[split_name].filter(lambda x: x['language'] == language_code and x['weat'] == weat_category_id)\n",
    "            if len(filtered) > 0:\n",
    "                return { 'targ1': filtered[0]['targ1.examples'], 'targ2': filtered[0]['targ2.examples'], 'attr1': filtered[0]['attr1.examples'], 'attr2': filtered[0]['attr2.examples'] }\n",
    "            else:\n",
    "                print(f\"Warning: No data for language '{language_code}' and category '{weat_category_id}'.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error filtering data for '{weat_category_id}' in language '{language_code}': {e}\")\n",
    "            return None\n",
    "\n",
    "class LayerEmbeddingExtractor:\n",
    "    \"\"\"Extracts hidden states from model layers.\"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embeddings(self, words: list, layer_idx: int):\n",
    "        \"\"\"Gets embeddings for a list of words at a specific layer.\"\"\"\n",
    "        all_embeddings = []\n",
    "        for word in words:\n",
    "            inputs = self.tokenizer(word, return_tensors=\"pt\", add_special_tokens=False).to(self.device)\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            word_embedding = outputs.hidden_states[layer_idx][0].mean(dim=0).float().cpu().numpy()\n",
    "            all_embeddings.append(word_embedding)\n",
    "        return np.array(all_embeddings)\n",
    "\n",
    "class BiasQuantifier:\n",
    "    \"\"\"Calculates bias scores using WEAT effect size.\"\"\"\n",
    "    def _s(self, w, A, B):\n",
    "        mean_cos_A = np.mean([cosine_similarity([w], [a])[0][0] for a in A])\n",
    "        mean_cos_B = np.mean([cosine_similarity([w], [b])[0][0] for b in B])\n",
    "        return mean_cos_A - mean_cos_B\n",
    "\n",
    "    def weat_effect_size(self, T1_embeds, T2_embeds, A1_embeds, A2_embeds):\n",
    "        \"\"\"Calculates the WEAT effect size (d-score).\"\"\"\n",
    "        mean_T1 = np.mean([self._s(t, A1_embeds, A2_embeds) for t in T1_embeds])\n",
    "        mean_T2 = np.mean([self._s(t, A1_embeds, A2_embeds) for t in T2_embeds])\n",
    "        all_s = [self._s(t, A1_embeds, A2_embeds) for t in np.concatenate((T1_embeds, T2_embeds))]\n",
    "        std_dev = np.std(all_s, ddof=1)\n",
    "        return (mean_T1 - mean_T2) / std_dev if std_dev > 0 else 0\n",
    "\n",
    "def create_detailed_comment(base_comment: str, language: str = \"Hindi\", dataset: str = \"alpaca\", model: str = \"SmolLM2_135M\", mode: str = None):\n",
    "    \"\"\"\n",
    "    Creates a detailed comment with task, language, dataset, model and parameters.\n",
    "    \n",
    "    Args:\n",
    "        base_comment (str): The base comment like \"Before_finetuning\" or \"After_epoch_1\"\n",
    "        language (str): The language being used\n",
    "        dataset (str): The dataset name\n",
    "        model (str): The model name (simplified)\n",
    "        mode (str): The execution mode (test/full), if applicable\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted detailed comment\n",
    "    \"\"\"\n",
    "    # Create the detailed comment\n",
    "    detailed_comment = f\"{base_comment}_on_{language}_{dataset}_{model}\"\n",
    "    \n",
    "    # Add mode if provided\n",
    "    if mode:\n",
    "        detailed_comment += f\"_{mode}\"\n",
    "        \n",
    "    return detailed_comment\n",
    "\n",
    "def execute_bias_analysis(model, tokenizer, results_path: str, hf_cache_dir: str, model_name: str, comments: str, languages: list, mode: str = None):\n",
    "    \"\"\"Runs the layer-wise bias analysis and saves the results.\"\"\"\n",
    "    weathub_loader = WEATHubLoader(dataset_id='iamshnoo/WEATHub', cache_dir=os.path.join(hf_cache_dir, \"datasets\"))\n",
    "    bias_quantifier = BiasQuantifier()\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    embedding_extractor = LayerEmbeddingExtractor(model, tokenizer)\n",
    "    all_results = []\n",
    "    weat_categories_to_test = ['WEAT1', 'WEAT2', 'WEAT6']\n",
    "    \n",
    "    # Create detailed comment\n",
    "    detailed_comment = create_detailed_comment(comments, mode=mode)\n",
    "\n",
    "    for lang in languages:\n",
    "        for weat_cat in weat_categories_to_test:\n",
    "            print(f\"\\nProcessing: Lang='{lang}', Category='{weat_cat}'\")\n",
    "            word_lists = weathub_loader.get_word_lists(lang, weat_cat)\n",
    "            if not word_lists: continue\n",
    "            for layer_idx in tqdm(range(num_layers), desc=f\"Layer Analysis ({lang}/{weat_cat})\"):\n",
    "                t1_embeds = embedding_extractor.get_embeddings(word_lists['targ1'], layer_idx)\n",
    "                t2_embeds = embedding_extractor.get_embeddings(word_lists['targ2'], layer_idx)\n",
    "                a1_embeds = embedding_extractor.get_embeddings(word_lists['attr1'], layer_idx)\n",
    "                a2_embeds = embedding_extractor.get_embeddings(word_lists['attr2'], layer_idx)\n",
    "                weat_score = bias_quantifier.weat_effect_size(t1_embeds, t2_embeds, a1_embeds, a2_embeds)\n",
    "                all_results.append({'model_id': model_name, 'language': lang, 'weat_category_id': weat_cat, 'layer_idx': layer_idx, 'weat_score': weat_score, 'comments': detailed_comment})\n",
    "\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        filename = f\"bias_results_{model_name.replace('/', '_')}_{detailed_comment.replace(' ', '_')}.csv\"\n",
    "        filepath = os.path.join(results_path, filename)\n",
    "        results_df.to_csv(filepath, index=False)\n",
    "        print(f\"Results successfully saved to: {filepath}\")\n",
    "    else:\n",
    "        print(\"No results were generated.\")\n",
    "    print(\"\\nAnalysis complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINE-TUNING AND UPLOAD COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "def create_prompt(example):\n",
    "    \"\"\"Creates a formatted instruction prompt from a dataset example.\"\"\"\n",
    "    template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    return template.format(instruction=example[\"instruction\"], output=example['output'] + \"</s>\")\n",
    "\n",
    "class BiasAnalysisCallback(TrainerCallback):\n",
    "    \"\"\"A custom TrainerCallback that runs bias analysis at the end of each epoch.\"\"\"\n",
    "    def __init__(self, tokenizer, results_path, hf_cache_dir, model_name, mode):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.results_path = results_path\n",
    "        self.hf_cache_dir = hf_cache_dir\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch = int(state.epoch)\n",
    "        model = kwargs['model']\n",
    "        print(f\"\\n--- Running Bias Analysis for Epoch {epoch} ---\")\n",
    "        execute_bias_analysis(model, self.tokenizer, self.results_path, self.hf_cache_dir, self.model_name, f\"After_epoch_{epoch}\", ['en', 'hi'], self.mode)\n",
    "        print(f\"--- Bias Analysis for Epoch {epoch} Completed ---\")\n",
    "\n",
    "def upload_to_hf(model_path, repo_name):\n",
    "    \"\"\"Uploads a model folder and associated artifacts to the Hugging Face Hub.\"\"\"\n",
    "    from huggingface_hub import HfApi, whoami\n",
    "    \n",
    "    print(f\"Starting upload of '{model_path}' to '{repo_name}'...\")\n",
    "    \n",
    "    # Quick authentication check\n",
    "    try:\n",
    "        user_info = whoami()\n",
    "        current_user = user_info.get('name')\n",
    "        print(f\"✅ Authenticated as: {current_user}\")\n",
    "        \n",
    "        # Verify the repo name matches the authenticated user\n",
    "        expected_user = repo_name.split('/')[0]\n",
    "        if current_user != expected_user:\n",
    "            print(f\"⚠️ WARNING: Authenticated user '{current_user}' doesn't match repo owner '{expected_user}'\")\n",
    "            repo_name = f\"{current_user}/SmolLM2-135M-finetuned-alpaca-hindi\"\n",
    "            print(f\"🔄 Using corrected repo name: {repo_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Authentication check failed: {e}\")\n",
    "        print(\"Please run the HuggingFace login cell first!\")\n",
    "        return False\n",
    "    \n",
    "    # Check if model directory exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model directory '{model_path}' not found!\")\n",
    "        return False\n",
    "    \n",
    "    # Attempt upload\n",
    "    try:\n",
    "        api = HfApi()\n",
    "        \n",
    "        # Create repository\n",
    "        print(f\"📝 Creating repository: {repo_name}\")\n",
    "        api.create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)\n",
    "        print(\"✅ Repository created/verified\")\n",
    "        \n",
    "        # Upload folder\n",
    "        print(f\"📤 Uploading folder: {model_path}\")\n",
    "        api.upload_folder(\n",
    "            folder_path=model_path, \n",
    "            repo_id=repo_name, \n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Upload fine-tuned SmolLM2-135M model for Hindi\"\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Upload completed successfully!\")\n",
    "        print(f\"🔗 View your model at: https://huggingface.co/{repo_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Upload failed with error: {e}\")\n",
    "        if \"401\" in str(e):\n",
    "            print(\"🚨 401 Unauthorized Error - Please run the HuggingFace login cell first!\")\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main function to orchestrate the fine-tuning and analysis process.\"\"\"\n",
    "    project_path, results_path, hf_cache_dir = setup_platform_environment(args.platform)\n",
    "    \n",
    "    # Create repo name with execution mode appended\n",
    "    repo_name_with_mode = f\"{NEW_MODEL_REPO_NAME}_{args.mode}\"\n",
    "\n",
    "    if args.action == 'upload':\n",
    "        if not os.path.exists(FINAL_MODEL_DIR):\n",
    "            print(f\"Error: Final model directory '{FINAL_MODEL_DIR}' not found. Please run training first.\")\n",
    "            return\n",
    "        upload_to_hf(FINAL_MODEL_DIR, repo_name_with_mode)\n",
    "        return\n",
    "\n",
    "    # --- Initial Bias Analysis (Before Fine-tuning) ---\n",
    "    print(\"\\n--- Running Initial Bias Analysis on Base Model ---\")\n",
    "    llm_manager = LLMManager(cache_dir=hf_cache_dir)\n",
    "    base_model, base_tokenizer = llm_manager.load_model(BASE_MODEL_NAME, 'hf')\n",
    "    if base_model and base_tokenizer:\n",
    "        execute_bias_analysis(base_model, base_tokenizer, results_path, hf_cache_dir, BASE_MODEL_NAME, \"Before_finetuning\", ['en'], args.mode)\n",
    "    llm_manager.unload_model()\n",
    "    print(\"--- Initial Bias Analysis Completed ---\")\n",
    "\n",
    "    # --- Fine-tuning ---\n",
    "    print(\"\\n--- Preparing for Fine-tuning ---\")\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dataset_with_prompt = dataset.map(lambda example: {\"text\": create_prompt(example)})\n",
    "    tokenized_dataset = dataset_with_prompt.map(lambda ex: tokenizer(ex[\"text\"], truncation=True, max_length=512), batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Change below line for full and final execution.\n",
    "    num_train_epochs = 1 if args.mode == 'test' else 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./SmolLM2-135M-hindi-tuned\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        # max_steps=-1 if args.mode == 'full' else 50,\n",
    "        max_steps=-1,\n",
    "        bf16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, torch_dtype=torch.bfloat16)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    if args.mode == 'test':\n",
    "        shuffled_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "        train_dataset, eval_dataset = shuffled_dataset.select(range(100)), shuffled_dataset.select(range(100, 120))\n",
    "    else:\n",
    "        split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        train_dataset, eval_dataset = split_dataset['train'], split_dataset['test']\n",
    "\n",
    "    bias_analysis_callback = BiasAnalysisCallback(tokenizer, results_path, hf_cache_dir, BASE_MODEL_NAME, args.mode)\n",
    "    # trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, data_collator=data_collator, tokenizer=tokenizer, callbacks=[bias_analysis_callback])\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, data_collator=data_collator, processing_class=tokenizer, callbacks=[bias_analysis_callback])\n",
    "\n",
    "    print(f\"--- Starting Fine-tuning (Mode: {args.mode}) ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Fine-tuning Completed ---\")\n",
    "\n",
    "    # Upload model to huggingface\n",
    "    trainer.save_model(FINAL_MODEL_DIR)\n",
    "    print(f\"Final model saved to {FINAL_MODEL_DIR}\")\n",
    "\n",
    "    # Automatically upload to HuggingFace Hub with mode appended\n",
    "    print(\"\\n--- Starting Automatic Upload to HuggingFace Hub ---\")\n",
    "    if os.path.exists(FINAL_MODEL_DIR):\n",
    "        upload_to_hf(FINAL_MODEL_DIR, repo_name_with_mode)\n",
    "        print(\"--- Upload to HuggingFace Hub Completed ---\")\n",
    "    else:\n",
    "        print(f\"Error: Final model directory '{FINAL_MODEL_DIR}' not found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    is_notebook = 'google.colab' in sys.modules or 'ipykernel' in sys.modules\n",
    "\n",
    "    if is_notebook:\n",
    "        print(\"Running in a notebook environment. Setting arguments manually.\")\n",
    "        # Change below line for full and final execution.\n",
    "        args = argparse.Namespace(action=\"train\", mode=\"full\", platform=\"digitalocean\")\n",
    "        main(args)\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description=\"Fine-tune and analyze SmolLM2-135M.\")\n",
    "        parser.add_argument(\"--action\", type=str, default=\"train\", choices=[\"train\", \"upload\"], help=\"Action to perform.\")\n",
    "        # Change below line for full and final execution.\n",
    "        parser.add_argument(\"--mode\", type=str, default=\"full\", choices=[\"test\", \"full\"], help=\"Training mode.\")\n",
    "        parser.add_argument(\"--platform\", type=str, default=\"local\", choices=[\"colab\", \"digitalocean\", \"local\"], help=\"Execution platform.\")\n",
    "        parsed_args = parser.parse_args()\n",
    "        main(parsed_args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
