{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09460cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script merges the functionalities of fine-tuning a language model and performing\n",
    "layer-wise bias analysis, including the final step of uploading the model to the\n",
    "Hugging Face Hub. It is designed to be run on platforms like Google Colab or\n",
    "DigitalOcean.\n",
    "\n",
    "Workflow:\n",
    "1.  **Initial Bias Analysis**: Performs a layer-wise bias analysis on the base\n",
    "    model (`HuggingFaceTB/SmolLM2-135M`) using an English dataset before any fine-tuning.\n",
    "2.  **Fine-tuning**: Fine-tunes the `HuggingFaceTB/SmolLM2-135M` model on a Hindi dataset\n",
    "    (`iamshnoo/alpaca-cleaned-hindi`).\n",
    "3.  **Post-Epoch Bias Analysis**: After each fine-tuning epoch, the script runs the\n",
    "    layer-wise bias analysis on the model for both English and Hindi to track how\n",
    "    bias evolves.\n",
    "4.  **Results**: All bias analysis results are saved to CSV files for further examination.\n",
    "5.  **Upload**: After training, the script can be run again with the 'upload' action\n",
    "    to push the final model, tokenizer, and training artifacts to the Hugging Face Hub.\n",
    "\n",
    "This script can be executed with command-line arguments to specify the action\n",
    "('train' or 'upload'), training mode ('test' or 'full'), and the platform\n",
    "('colab' or 'local').\n",
    "\"\"\"\n",
    "\n",
    "# In a notebook environment (like Colab), run this cell first to install dependencies.\n",
    "# !pip install -q transformers>=4.32.0 datasets torch pandas numpy scikit-learn accelerate bitsandbytes\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, login\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# =============================================================================\n",
    "BASE_MODEL_NAME = \"Debk/SmolLM2-135M-finetuned-alpaca-hindi\"  # Pre-trained model from HuggingFace\n",
    "DATASET_NAME = \"iamshnoo/alpaca-cleaned-bengali\"  # Bengali dataset\n",
    "HF_USERNAME = \"Debk\"  # <-- IMPORTANT: SET YOUR USERNAME HERE\n",
    "NEW_MODEL_REPO_NAME = f\"{HF_USERNAME}/SmolLM2-135M-finetuned-alpaca-hindi-bengali\"\n",
    "FINAL_MODEL_DIR = \"./SmolLM2-135M-hindi-bengali-final\"\n",
    "BIAS_RESULTS_DIR = \"./bias_analysis_results\"\n",
    "\n",
    "# Language mapping for WEATHub dataset codes vs full names\n",
    "LANGUAGE_MAPPING = {\n",
    "    'english': 'en',\n",
    "    'hindi': 'hi', \n",
    "    'bengali': 'bn'\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# PLATFORM-SPECIFIC CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def setup_platform_environment(platform: str = \"local\"):\n",
    "    \"\"\"\n",
    "    Configures the environment based on the specified platform.\n",
    "\n",
    "    Args:\n",
    "        platform (str): The target platform, e.g., 'colab', 'digitalocean', 'local'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (project_path, results_path, hf_cache_dir).\n",
    "    \"\"\"\n",
    "    print(f\"Setting up environment for: {platform.upper()}\")\n",
    "\n",
    "    if platform == \"colab\":\n",
    "        try:\n",
    "            from google.colab import drive, userdata\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive mounted successfully.\")\n",
    "\n",
    "            project_path = \"/content/drive/MyDrive/Mult_LLM_Bias/\"\n",
    "            results_path = os.path.join(project_path, \"Results/Experiment1/\")\n",
    "            hf_cache_dir = os.path.join(project_path, \"hf_cache/\")\n",
    "\n",
    "            print(\"Attempting to log in to Hugging Face...\")\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if HF_TOKEN:\n",
    "                login(HF_TOKEN, add_to_git_credential=True)\n",
    "                print(\"Successfully logged in to Hugging Face!\")\n",
    "            else:\n",
    "                print(\"Hugging Face token not found in Colab secrets.\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"Could not import Google Colab libraries. Defaulting to local setup.\")\n",
    "            return setup_platform_environment(\"local\")\n",
    "\n",
    "    elif platform == \"digitalocean\":\n",
    "        print(\"Setting up DigitalOcean environment...\")\n",
    "        \n",
    "        # Set paths for DigitalOcean - using root directory as base\n",
    "        project_path = \"/root/Mult_LLM_Bias/\"\n",
    "        results_path = os.path.join(project_path, \"Results/Experiment1/\")\n",
    "        hf_cache_dir = os.path.join(project_path, \"hf_cache/\")\n",
    "        \n",
    "        # Check if block storage volume is available (optional)\n",
    "        volume_path = \"/mnt/volume_nyc1_01/\"\n",
    "        if os.path.exists(volume_path):\n",
    "            print(\"DigitalOcean volume detected. Using volume storage...\")\n",
    "            project_path = os.path.join(volume_path, \"Mult_LLM_Bias/\")\n",
    "            results_path = os.path.join(project_path, \"Results/Experiment1/\")\n",
    "            hf_cache_dir = os.path.join(project_path, \"hf_cache/\")\n",
    "        else:\n",
    "            print(\"Using root directory storage...\")\n",
    "        \n",
    "        # Set up Hugging Face authentication for DigitalOcean\n",
    "        print(\"Setting up Hugging Face authentication...\")\n",
    "        \n",
    "        # First try environment variable\n",
    "        hf_token = os.environ.get('HF_TOKEN')\n",
    "        if not hf_token:\n",
    "            # Set your token\n",
    "            hf_token = \"hf_secret\"  # Replace with your actual Hugging Face token\n",
    "            # Set it as environment variable for this session\n",
    "            os.environ['HF_TOKEN'] = hf_token\n",
    "        \n",
    "        try:\n",
    "            login(hf_token, add_to_git_credential=True)\n",
    "            print(\"‚úÖ Successfully logged in to Hugging Face!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to login to Hugging Face: {e}\")\n",
    "            print(\"Please check your token and internet connection.\")\n",
    "        \n",
    "        # Optimize for DigitalOcean environment\n",
    "        os.environ['TRANSFORMERS_CACHE'] = hf_cache_dir\n",
    "        os.environ['HF_HOME'] = hf_cache_dir\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU if available\n",
    "        \n",
    "        print(\"DigitalOcean environment configured successfully.\")\n",
    "\n",
    "    else: # Default to local setup\n",
    "        project_path = \"./\"\n",
    "        results_path = BIAS_RESULTS_DIR\n",
    "        hf_cache_dir = \"./hf_cache/\"\n",
    "\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Project path set to: {project_path}\")\n",
    "    print(f\"Bias analysis results will be saved to: {results_path}\")\n",
    "\n",
    "    return project_path, results_path, hf_cache_dir\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LAYER-WISE BIAS ANALYSIS COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "class LLMManager:\n",
    "    \"\"\"Manages the lifecycle of LLMs to optimize memory usage.\"\"\"\n",
    "    def __init__(self, cache_dir: str):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.current_model_id = None\n",
    "\n",
    "    def load_model(self, model_id: str, model_repo: str):\n",
    "        \"\"\"Load model and tokenizer from either Hugging Face or a local path.\"\"\"\n",
    "        if self.current_model_id == model_id and self.model is not None:\n",
    "            print(f\"Model '{model_id}' already loaded.\")\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        print(f\"Loading model: {model_id} from {model_repo}\")\n",
    "        load_path = model_id if model_repo == 'hf' else model_repo\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(load_path, cache_dir=self.cache_dir)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            #Update Config\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "             )\n",
    "\n",
    "            # quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                load_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                cache_dir=self.cache_dir\n",
    "            )\n",
    "            self.current_model_id = model_id\n",
    "            print(f\"Model '{model_id}' loaded successfully.\")\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load model '{model_id}'. Exception: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Unloads the model and clears GPU cache.\"\"\"\n",
    "        if self.model:\n",
    "            print(f\"Unloading model: {self.current_model_id}...\")\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            self.model, self.tokenizer, self.current_model_id = None, None, None\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Model unloaded and memory cleared.\")\n",
    "\n",
    "class WEATHubLoader:\n",
    "    \"\"\"Loads the WEATHub dataset and provides word lists.\"\"\"\n",
    "    def __init__(self, dataset_id: str, cache_dir: str = None):\n",
    "        print(f\"Loading WEATHub dataset from '{dataset_id}'...\")\n",
    "        try:\n",
    "            self.dataset = load_dataset(dataset_id, cache_dir=cache_dir)\n",
    "            print(\"WEATHub dataset loaded successfully.\")\n",
    "            self.split_mapping = {\n",
    "                'WEAT1': 'original_weat', 'WEAT2': 'original_weat', 'WEAT6': 'original_weat', 'WEAT7': 'original_weat', 'WEAT8': 'original_weat'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load WEATHub dataset. Exception: {e}\")\n",
    "            self.dataset = None\n",
    "\n",
    "    def get_word_lists(self, language_code: str, weat_category_id: str):\n",
    "        \"\"\"Retrieves target and attribute word lists.\"\"\"\n",
    "        if not self.dataset: return None\n",
    "        split_name = self.split_mapping.get(weat_category_id)\n",
    "        if not split_name:\n",
    "            print(f\"Warning: Category '{weat_category_id}' not found.\")\n",
    "            return None\n",
    "        try:\n",
    "            filtered = self.dataset[split_name].filter(lambda x: x['language'] == language_code and x['weat'] == weat_category_id)\n",
    "            if len(filtered) > 0:\n",
    "                return { 'targ1': filtered[0]['targ1.examples'], 'targ2': filtered[0]['targ2.examples'], 'attr1': filtered[0]['attr1.examples'], 'attr2': filtered[0]['attr2.examples'] }\n",
    "            else:\n",
    "                print(f\"Warning: No data for language '{language_code}' and category '{weat_category_id}'.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error filtering data for '{weat_category_id}' in language '{language_code}': {e}\")\n",
    "            return None\n",
    "\n",
    "class LayerEmbeddingExtractor:\n",
    "    \"\"\"Extracts hidden states from model layers.\"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embeddings(self, words: list, layer_idx: int):\n",
    "        \"\"\"Gets embeddings for a list of words at a specific layer.\"\"\"\n",
    "        all_embeddings = []\n",
    "        for word in words:\n",
    "            inputs = self.tokenizer(word, return_tensors=\"pt\", add_special_tokens=False).to(self.device)\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            word_embedding = outputs.hidden_states[layer_idx][0].mean(dim=0).float().cpu().numpy()\n",
    "            all_embeddings.append(word_embedding)\n",
    "        return np.array(all_embeddings)\n",
    "\n",
    "class BiasQuantifier:\n",
    "    \"\"\"Calculates bias scores using WEAT effect size.\"\"\"\n",
    "    def _s(self, w, A, B):\n",
    "        mean_cos_A = np.mean([cosine_similarity([w], [a])[0][0] for a in A])\n",
    "        mean_cos_B = np.mean([cosine_similarity([w], [b])[0][0] for b in B])\n",
    "        return mean_cos_A - mean_cos_B\n",
    "\n",
    "    def weat_effect_size(self, T1_embeds, T2_embeds, A1_embeds, A2_embeds):\n",
    "        \"\"\"Calculates the WEAT effect size (d-score).\"\"\"\n",
    "        mean_T1 = np.mean([self._s(t, A1_embeds, A2_embeds) for t in T1_embeds])\n",
    "        mean_T2 = np.mean([self._s(t, A1_embeds, A2_embeds) for t in T2_embeds])\n",
    "        all_s = [self._s(t, A1_embeds, A2_embeds) for t in np.concatenate((T1_embeds, T2_embeds))]\n",
    "        std_dev = np.std(all_s, ddof=1)\n",
    "        return (mean_T1 - mean_T2) / std_dev if std_dev > 0 else 0\n",
    "\n",
    "def create_detailed_comment(base_comment: str, language: str = \"bengali\", dataset: str = \"alpaca\", model: str = \"SmolLM2-135M-finetuned-alpaca-hindi\", mode: str = None):\n",
    "    \"\"\"\n",
    "    Creates a detailed comment with task, language, dataset, model and parameters.\n",
    "    \n",
    "    Args:\n",
    "        base_comment (str): The base comment like \"Before_bengali_finetuning\" or \"After_epoch_1\"\n",
    "        language (str): The language being used for finetuning\n",
    "        dataset (str): The dataset name\n",
    "        model (str): The model name (simplified)\n",
    "        mode (str): The execution mode (test/full), if applicable\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted detailed comment\n",
    "    \"\"\"\n",
    "    # Create the detailed comment with proper formatting\n",
    "    if \"Before\" in base_comment:\n",
    "        detailed_comment = f\"{base_comment}_on_{model}\"\n",
    "    else:\n",
    "        detailed_comment = f\"{base_comment}_{language}_finetune_on_{model}\"\n",
    "    \n",
    "    # Add mode if provided\n",
    "    if mode:\n",
    "        detailed_comment += f\"_{mode}\"\n",
    "        \n",
    "    return detailed_comment\n",
    "\n",
    "def execute_bias_analysis(model, tokenizer, results_path: str, hf_cache_dir: str, model_name: str, comments: str, languages: list, mode: str = None):\n",
    "    \"\"\"Runs the layer-wise bias analysis and saves the results.\"\"\"\n",
    "    weathub_loader = WEATHubLoader(dataset_id='iamshnoo/WEATHub', cache_dir=os.path.join(hf_cache_dir, \"datasets\"))\n",
    "    bias_quantifier = BiasQuantifier()\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    embedding_extractor = LayerEmbeddingExtractor(model, tokenizer)\n",
    "    all_results = []\n",
    "    weat_categories_to_test = ['WEAT1', 'WEAT2', 'WEAT6']\n",
    "    \n",
    "    # Create detailed comment\n",
    "    detailed_comment = create_detailed_comment(comments, mode=mode)\n",
    "\n",
    "    for lang_full in languages:\n",
    "        # Map full language names to WEATHub codes\n",
    "        lang_code = LANGUAGE_MAPPING.get(lang_full, lang_full)\n",
    "        \n",
    "        for weat_cat in weat_categories_to_test:\n",
    "            print(f\"\\nProcessing: Lang='{lang_full}' (code: {lang_code}), Category='{weat_cat}'\")\n",
    "            word_lists = weathub_loader.get_word_lists(lang_code, weat_cat)\n",
    "            if not word_lists: \n",
    "                print(f\"No word lists found for {lang_full} ({lang_code}) and {weat_cat}\")\n",
    "                continue\n",
    "            for layer_idx in tqdm(range(num_layers), desc=f\"Layer Analysis ({lang_full}/{weat_cat})\"):\n",
    "                t1_embeds = embedding_extractor.get_embeddings(word_lists['targ1'], layer_idx)\n",
    "                t2_embeds = embedding_extractor.get_embeddings(word_lists['targ2'], layer_idx)\n",
    "                a1_embeds = embedding_extractor.get_embeddings(word_lists['attr1'], layer_idx)\n",
    "                a2_embeds = embedding_extractor.get_embeddings(word_lists['attr2'], layer_idx)\n",
    "                weat_score = bias_quantifier.weat_effect_size(t1_embeds, t2_embeds, a1_embeds, a2_embeds)\n",
    "                # Store full language name in results\n",
    "                all_results.append({\n",
    "                    'model_id': model_name, \n",
    "                    'language': lang_full,  # Store full language name\n",
    "                    'weat_category_id': weat_cat, \n",
    "                    'layer_idx': layer_idx, \n",
    "                    'weat_score': weat_score, \n",
    "                    'comments': detailed_comment\n",
    "                })\n",
    "\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        filename = f\"bias_results_{model_name.replace('/', '_')}_{detailed_comment.replace(' ', '_')}.csv\"\n",
    "        filepath = os.path.join(results_path, filename)\n",
    "        results_df.to_csv(filepath, index=False)\n",
    "        print(f\"Results successfully saved to: {filepath}\")\n",
    "    else:\n",
    "        print(\"No results were generated.\")\n",
    "    print(\"\\nAnalysis complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINE-TUNING AND UPLOAD COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "def create_prompt(example):\n",
    "    \"\"\"Creates a formatted instruction prompt from a dataset example.\"\"\"\n",
    "    template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    return template.format(instruction=example[\"instruction\"], output=example['output'] + \"</s>\")\n",
    "\n",
    "class BiasAnalysisCallback(TrainerCallback):\n",
    "    \"\"\"A custom TrainerCallback that runs bias analysis at the end of each epoch.\"\"\"\n",
    "    def __init__(self, tokenizer, results_path, hf_cache_dir, model_name, mode):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.results_path = results_path\n",
    "        self.hf_cache_dir = hf_cache_dir\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch = int(state.epoch)\n",
    "        model = kwargs['model']\n",
    "        print(f\"\\n--- Running Bias Analysis for Epoch {epoch} ---\")\n",
    "        # Analyze bias for Hindi, English, and Bengali after each epoch\n",
    "        execute_bias_analysis(model, self.tokenizer, self.results_path, self.hf_cache_dir, self.model_name, f\"After_epoch_{epoch}\", ['hindi', 'english', 'bengali'], self.mode)\n",
    "        print(f\"--- Bias Analysis for Epoch {epoch} Completed ---\")\n",
    "\n",
    "def upload_to_hf(model_path, repo_name):\n",
    "    \"\"\"Uploads a model folder and associated artifacts to the Hugging Face Hub.\"\"\"\n",
    "    from huggingface_hub import HfApi, whoami\n",
    "    \n",
    "    print(f\"Starting upload of '{model_path}' to '{repo_name}'...\")\n",
    "    \n",
    "    # Quick authentication check\n",
    "    try:\n",
    "        user_info = whoami()\n",
    "        current_user = user_info.get('name')\n",
    "        print(f\"‚úÖ Authenticated as: {current_user}\")\n",
    "        \n",
    "        # Verify the repo name matches the authenticated user\n",
    "        expected_user = repo_name.split('/')[0]\n",
    "        if current_user != expected_user:\n",
    "            print(f\"‚ö†Ô∏è WARNING: Authenticated user '{current_user}' doesn't match repo owner '{expected_user}'\")\n",
    "            repo_name = f\"{current_user}/SmolLM2-135M-finetuned-alpaca-hindi\"\n",
    "            print(f\"üîÑ Using corrected repo name: {repo_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Authentication check failed: {e}\")\n",
    "        print(\"Please run the HuggingFace login cell first!\")\n",
    "        return False\n",
    "    \n",
    "    # Check if model directory exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ùå Model directory '{model_path}' not found!\")\n",
    "        return False\n",
    "    \n",
    "    # Attempt upload\n",
    "    try:\n",
    "        api = HfApi()\n",
    "        \n",
    "        # Create repository\n",
    "        print(f\"üìù Creating repository: {repo_name}\")\n",
    "        api.create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)\n",
    "        print(\"‚úÖ Repository created/verified\")\n",
    "        \n",
    "        # Upload folder\n",
    "        print(f\"üì§ Uploading folder: {model_path}\")\n",
    "        api.upload_folder(\n",
    "            folder_path=model_path, \n",
    "            repo_id=repo_name, \n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Upload fine-tuned SmolLM2-135M model for Hindi\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Upload completed successfully!\")\n",
    "        print(f\"üîó View your model at: https://huggingface.co/{repo_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload failed with error: {e}\")\n",
    "        if \"401\" in str(e):\n",
    "            print(\"üö® 401 Unauthorized Error - Please run the HuggingFace login cell first!\")\n",
    "        return False\n",
    "\n",
    "def merge_all_csv_results(results_path: str, output_filename: str = \"merged_bias_results.csv\"):\n",
    "    \"\"\"\n",
    "    Merges all CSV files from the results directory into a single CSV file.\n",
    "    \n",
    "    Args:\n",
    "        results_path (str): Path to the directory containing CSV files\n",
    "        output_filename (str): Name of the output merged CSV file\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the merged CSV file\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    print(f\"\\n--- Merging CSV Results from {results_path} ---\")\n",
    "    \n",
    "    # Find all CSV files in the results directory\n",
    "    csv_pattern = os.path.join(results_path, \"bias_results_*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found matching pattern: {csv_pattern}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to merge:\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"  - {os.path.basename(csv_file)}\")\n",
    "    \n",
    "    # Read and concatenate all CSV files\n",
    "    all_dataframes = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            print(f\"Loaded {len(df)} rows from {os.path.basename(csv_file)}\")\n",
    "            all_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"No valid CSV files could be loaded.\")\n",
    "        return None\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Save merged results\n",
    "    merged_filepath = os.path.join(results_path, output_filename)\n",
    "    merged_df.to_csv(merged_filepath, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully merged {len(csv_files)} CSV files\")\n",
    "    print(f\"üìä Total rows in merged file: {len(merged_df)}\")\n",
    "    print(f\"üíæ Merged results saved to: {merged_filepath}\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"\\nüìà Summary of merged results:\")\n",
    "    print(f\"  - Unique models: {merged_df['model_id'].nunique()}\")\n",
    "    print(f\"  - Languages analyzed: {merged_df['language'].unique().tolist()}\")\n",
    "    print(f\"  - WEAT categories: {merged_df['weat_category_id'].unique().tolist()}\")\n",
    "    print(f\"  - Analysis stages: {merged_df['comments'].nunique()}\")\n",
    "    \n",
    "    return merged_filepath\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main function to orchestrate the Bengali fine-tuning and analysis process.\"\"\"\n",
    "    project_path, results_path, hf_cache_dir = setup_platform_environment(args.platform)\n",
    "    \n",
    "    # Create repo name with execution mode appended\n",
    "    repo_name_with_mode = f\"{NEW_MODEL_REPO_NAME}_{args.mode}\"\n",
    "\n",
    "    if args.action == 'upload':\n",
    "        if not os.path.exists(FINAL_MODEL_DIR):\n",
    "            print(f\"Error: Final model directory '{FINAL_MODEL_DIR}' not found. Please run training first.\")\n",
    "            return\n",
    "        upload_to_hf(FINAL_MODEL_DIR, repo_name_with_mode)\n",
    "        return\n",
    "\n",
    "    # --- Initial Bias Analysis (Before Bengali Fine-tuning) ---\n",
    "    print(f\"\\n--- Running Initial Bias Analysis on Pre-trained Model: {BASE_MODEL_NAME} ---\")\n",
    "    llm_manager = LLMManager(cache_dir=hf_cache_dir)\n",
    "    base_model, base_tokenizer = llm_manager.load_model(BASE_MODEL_NAME, 'hf')\n",
    "    if base_model and base_tokenizer:\n",
    "        # Analyze bias for Hindi and English before Bengali finetuning\n",
    "        execute_bias_analysis(base_model, base_tokenizer, results_path, hf_cache_dir, BASE_MODEL_NAME, \"Before_bengali_finetuning\", ['hindi', 'english'], args.mode)\n",
    "    llm_manager.unload_model()\n",
    "    print(\"--- Initial Bias Analysis Completed ---\")\n",
    "\n",
    "    # --- Fine-tuning on Bengali Dataset ---\n",
    "    print(f\"\\n--- Preparing for Bengali Fine-tuning ---\")\n",
    "    print(f\"Loading Bengali dataset: {DATASET_NAME}\")\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dataset_with_prompt = dataset.map(lambda example: {\"text\": create_prompt(example)})\n",
    "    tokenized_dataset = dataset_with_prompt.map(lambda ex: tokenizer(ex[\"text\"], truncation=True, max_length=512), batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Change below line for full and final execution.\n",
    "    num_train_epochs = 1 if args.mode == 'test' else 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./SmolLM2-135M-hindi-bengali-tuned\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        # max_steps=-1 if args.mode == 'full' else 50,\n",
    "        max_steps=-1,\n",
    "        bf16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    print(f\"Loading pre-trained model: {BASE_MODEL_NAME}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, torch_dtype=torch.bfloat16)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    if args.mode == 'test':\n",
    "        shuffled_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "        train_dataset, eval_dataset = shuffled_dataset.select(range(100)), shuffled_dataset.select(range(100, 120))\n",
    "    else:\n",
    "        split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        train_dataset, eval_dataset = split_dataset['train'], split_dataset['test']\n",
    "\n",
    "    bias_analysis_callback = BiasAnalysisCallback(tokenizer, results_path, hf_cache_dir, BASE_MODEL_NAME, args.mode)\n",
    "    # trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, data_collator=data_collator, tokenizer=tokenizer, callbacks=[bias_analysis_callback])\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, data_collator=data_collator, processing_class=tokenizer, callbacks=[bias_analysis_callback])\n",
    "\n",
    "    print(f\"--- Starting Bengali Fine-tuning (Mode: {args.mode}) ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Bengali Fine-tuning Completed ---\")\n",
    "\n",
    "    # Save final model\n",
    "    trainer.save_model(FINAL_MODEL_DIR)\n",
    "    print(f\"Final Bengali-finetuned model saved to {FINAL_MODEL_DIR}\")\n",
    "\n",
    "    # Merge all CSV results\n",
    "    print(\"\\n--- Merging All Bias Analysis Results ---\")\n",
    "    merged_csv_path = merge_all_csv_results(results_path, f\"merged_bias_results_{args.mode}.csv\")\n",
    "    if merged_csv_path:\n",
    "        print(f\"‚úÖ All results merged successfully: {merged_csv_path}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to merge CSV results\")\n",
    "\n",
    "    # Automatically upload to HuggingFace Hub with mode appended\n",
    "    print(\"\\n--- Starting Automatic Upload to HuggingFace Hub ---\")\n",
    "    if os.path.exists(FINAL_MODEL_DIR):\n",
    "        upload_to_hf(FINAL_MODEL_DIR, repo_name_with_mode)\n",
    "        print(\"--- Upload to HuggingFace Hub Completed ---\")\n",
    "    else:\n",
    "        print(f\"Error: Final model directory '{FINAL_MODEL_DIR}' not found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    is_notebook = 'google.colab' in sys.modules or 'ipykernel' in sys.modules\n",
    "\n",
    "    if is_notebook:\n",
    "        print(\"Running in a notebook environment. Setting arguments manually.\")\n",
    "        # Change below line for full and final execution.\n",
    "        args = argparse.Namespace(action=\"train\", mode=\"full\", platform=\"digitalocean\")\n",
    "        main(args)\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description=\"Fine-tune and analyze SmolLM2-135M.\")\n",
    "        parser.add_argument(\"--action\", type=str, default=\"train\", choices=[\"train\", \"upload\"], help=\"Action to perform.\")\n",
    "        # Change below line for full and final execution.\n",
    "        parser.add_argument(\"--mode\", type=str, default=\"full\", choices=[\"test\", \"full\"], help=\"Training mode.\")\n",
    "        parser.add_argument(\"--platform\", type=str, default=\"local\", choices=[\"colab\", \"digitalocean\", \"local\"], help=\"Execution platform.\")\n",
    "        parsed_args = parser.parse_args()\n",
    "        main(parsed_args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
